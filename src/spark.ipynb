{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componente de machine learning do Spark MLlib\n",
    "* Acesso a dados\n",
    "* Regressão, classificação e agrupamento com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ponto de entrada - sessão spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exemplo de operação com dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|idade|  nome|\n",
      "+-----+------+\n",
      "| null|Silvio|\n",
      "|   30| teste|\n",
      "|   19|teste2|\n",
      "|   13|teste3|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"pessoas.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algumas operações com dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idade: long (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|  nome|\n",
      "+------+\n",
      "|Silvio|\n",
      "| teste|\n",
      "|teste2|\n",
      "|teste3|\n",
      "+------+\n",
      "\n",
      "+------+-----------+\n",
      "|  nome|(idade + 1)|\n",
      "+------+-----------+\n",
      "|Silvio|       null|\n",
      "| teste|         31|\n",
      "|teste2|         20|\n",
      "|teste3|         14|\n",
      "+------+-----------+\n",
      "\n",
      "+-----+-----+\n",
      "|idade| nome|\n",
      "+-----+-----+\n",
      "|   30|teste|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|idade|count|\n",
      "+-----+-----+\n",
      "|   19|    1|\n",
      "| null|    1|\n",
      "|   13|    1|\n",
      "|   30|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema\n",
    "df.printSchema()\n",
    "\n",
    "# selecione apenas a coluna nome\n",
    "df.select(\"nome\").show()\n",
    "\n",
    "# seleciona todo mundo e adiciona 1 a idade\n",
    "df.select(df['nome'], df['idade'] + 1).show()\n",
    "\n",
    "# filtra apenas maiores de 21\n",
    "df.filter(df['idade'] > 21).show()\n",
    "\n",
    "# Conta pessoas por idade\n",
    "df.groupBy(\"idade\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Preparação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|year|origin|                name|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "|18.0|        8|       307.0|       130|  3504|        12.0|  70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|       165|  3693|        11.5|  70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0|       150|  3436|        11.0|  70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0|       150|  3433|        12.0|  70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0|       140|  3449|        10.5|  70|     1|         ford torino|\n",
      "|15.0|        8|       429.0|       198|  4341|        10.0|  70|     1|    ford galaxie 500|\n",
      "|14.0|        8|       454.0|       220|  4354|         9.0|  70|     1|    chevrolet impala|\n",
      "|14.0|        8|       440.0|       215|  4312|         8.5|  70|     1|   plymouth fury iii|\n",
      "|14.0|        8|       455.0|       225|  4425|        10.0|  70|     1|    pontiac catalina|\n",
      "|15.0|        8|       390.0|       190|  3850|         8.5|  70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|       383.0|       170|  3563|        10.0|  70|     1| dodge challenger se|\n",
      "|14.0|        8|       340.0|       160|  3609|         8.0|  70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|       400.0|       150|  3761|         9.5|  70|     1|chevrolet monte c...|\n",
      "|14.0|        8|       455.0|       225|  3086|        10.0|  70|     1|buick estate wago...|\n",
      "|24.0|        4|       113.0|        95|  2372|        15.0|  70|     3|toyota corona mar...|\n",
      "|22.0|        6|       198.0|        95|  2833|        15.5|  70|     1|     plymouth duster|\n",
      "|18.0|        6|       199.0|        97|  2774|        15.5|  70|     1|          amc hornet|\n",
      "|21.0|        6|       200.0|        85|  2587|        16.0|  70|     1|       ford maverick|\n",
      "|27.0|        4|        97.0|        88|  2130|        14.5|  70|     3|        datsun pl510|\n",
      "|26.0|        4|        97.0|        46|  1835|        20.5|  70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#carregando CSV\n",
    "training = spark.read.load(\"Auto2.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Indexando para coluna categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+----+------+--------------------+----------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|year|origin|                name|origin_cat|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+----------+\n",
      "|18.0|        8|       307.0|       130|  3504|        12.0|  70|     1|chevrolet chevell...|       0.0|\n",
      "|15.0|        8|       350.0|       165|  3693|        11.5|  70|     1|   buick skylark 320|       0.0|\n",
      "|18.0|        8|       318.0|       150|  3436|        11.0|  70|     1|  plymouth satellite|       0.0|\n",
      "|16.0|        8|       304.0|       150|  3433|        12.0|  70|     1|       amc rebel sst|       0.0|\n",
      "|17.0|        8|       302.0|       140|  3449|        10.5|  70|     1|         ford torino|       0.0|\n",
      "|15.0|        8|       429.0|       198|  4341|        10.0|  70|     1|    ford galaxie 500|       0.0|\n",
      "|14.0|        8|       454.0|       220|  4354|         9.0|  70|     1|    chevrolet impala|       0.0|\n",
      "|14.0|        8|       440.0|       215|  4312|         8.5|  70|     1|   plymouth fury iii|       0.0|\n",
      "|14.0|        8|       455.0|       225|  4425|        10.0|  70|     1|    pontiac catalina|       0.0|\n",
      "|15.0|        8|       390.0|       190|  3850|         8.5|  70|     1|  amc ambassador dpl|       0.0|\n",
      "|15.0|        8|       383.0|       170|  3563|        10.0|  70|     1| dodge challenger se|       0.0|\n",
      "|14.0|        8|       340.0|       160|  3609|         8.0|  70|     1|  plymouth 'cuda 340|       0.0|\n",
      "|15.0|        8|       400.0|       150|  3761|         9.5|  70|     1|chevrolet monte c...|       0.0|\n",
      "|14.0|        8|       455.0|       225|  3086|        10.0|  70|     1|buick estate wago...|       0.0|\n",
      "|24.0|        4|       113.0|        95|  2372|        15.0|  70|     3|toyota corona mar...|       1.0|\n",
      "|22.0|        6|       198.0|        95|  2833|        15.5|  70|     1|     plymouth duster|       0.0|\n",
      "|18.0|        6|       199.0|        97|  2774|        15.5|  70|     1|          amc hornet|       0.0|\n",
      "|21.0|        6|       200.0|        85|  2587|        16.0|  70|     1|       ford maverick|       0.0|\n",
      "|27.0|        4|        97.0|        88|  2130|        14.5|  70|     3|        datsun pl510|       1.0|\n",
      "|26.0|        4|        97.0|        46|  1835|        20.5|  70|     2|volkswagen 1131 d...|       2.0|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer=StringIndexer(inputCol='origin',outputCol='origin_cat')\n",
    "indexed=indexer.fit(training).transform(training)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Separando entre features e target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features| mpg|\n",
      "+--------------------+----+\n",
      "|[8.0,307.0,130.0,...|18.0|\n",
      "|[8.0,350.0,165.0,...|15.0|\n",
      "|[8.0,318.0,150.0,...|18.0|\n",
      "|[8.0,304.0,150.0,...|16.0|\n",
      "|[8.0,302.0,140.0,...|17.0|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#input cols identifica features\n",
    "assembler=VectorAssembler(inputCols=['cylinders','displacement','horsepower','weight','acceleration','year','origin_cat']\n",
    "                          ,outputCol='features')\n",
    "\n",
    "output=assembler.transform(indexed)\n",
    "output.select('features','mpg').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Separa em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              mpg|\n",
      "+-------+-----------------+\n",
      "|  count|              274|\n",
      "|   mean|23.67956204379561|\n",
      "| stddev|7.725175424456545|\n",
      "|    min|              9.0|\n",
      "|    max|             46.6|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_data=output.select('features','mpg')\n",
    "train_data,test_data=final_data.randomSplit([0.7,0.3])\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Executa o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "mpg does not exist. Available: features, Class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-dfcd64011063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fiep-2021/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.conda/envs/fiep-2021/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fiep-2021/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fiep-2021/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fiep-2021/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: mpg does not exist. Available: features, Class"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr=LinearRegression(featuresCol='features',labelCol='mpg')\n",
    "\n",
    "model=lr.fit(train_data)\n",
    "\n",
    "results=model.evaluate(train_data)\n",
    "  \n",
    "print('Rsquared :',results.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[3.0,70.0,90.0,21...|\n",
      "|[3.0,70.0,97.0,23...|\n",
      "|[4.0,71.0,65.0,17...|\n",
      "|[4.0,79.0,58.0,18...|\n",
      "|[4.0,79.0,67.0,19...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data=test_data.select('features')\n",
    "unlabeled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[3.0,70.0,90.0,21...| 25.78089340728376|\n",
      "|[3.0,70.0,97.0,23...|23.600437132038923|\n",
      "|[4.0,71.0,65.0,17...| 27.58818941678378|\n",
      "|[4.0,79.0,58.0,18...| 33.58813799313259|\n",
      "|[4.0,79.0,67.0,19...| 29.68429055520404|\n",
      "|[4.0,85.0,65.0,21...|32.584337768452656|\n",
      "|[4.0,86.0,64.0,18...| 32.74858968574691|\n",
      "|[4.0,86.0,65.0,19...| 31.92343594705451|\n",
      "|[4.0,86.0,65.0,20...| 32.63943820615337|\n",
      "|[4.0,88.0,76.0,20...|26.595923166374956|\n",
      "|[4.0,90.0,75.0,21...|  28.8561723018591|\n",
      "|[4.0,91.0,67.0,18...| 33.28960735033425|\n",
      "|[4.0,91.0,67.0,19...| 34.31015068793157|\n",
      "|[4.0,91.0,67.0,19...| 34.44825640843146|\n",
      "|[4.0,91.0,67.0,19...| 34.35278277732755|\n",
      "|[4.0,97.0,54.0,22...| 28.34332578101012|\n",
      "|[4.0,97.0,67.0,20...| 33.56369068343143|\n",
      "|[4.0,97.0,78.0,19...| 32.13286850965254|\n",
      "|[4.0,98.0,63.0,20...|28.908510765843335|\n",
      "|[4.0,98.0,66.0,18...| 30.74784775898139|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=model.transform(unlabeled_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classificação com pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Class|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|    1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|    0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|    1|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|    0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|    1|\n",
      "|          5|    116|           74|            0|      0|25.6|                   0.201| 30|    0|\n",
      "|          3|     78|           50|           32|     88|31.0|                   0.248| 26|    1|\n",
      "|         10|    115|            0|            0|      0|35.3|                   0.134| 29|    0|\n",
      "|          2|    197|           70|           45|    543|30.5|                   0.158| 53|    1|\n",
      "|          8|    125|           96|            0|      0| 0.0|                   0.232| 54|    1|\n",
      "|          4|    110|           92|            0|      0|37.6|                   0.191| 30|    0|\n",
      "|         10|    168|           74|            0|      0|38.0|                   0.537| 34|    1|\n",
      "|         10|    139|           80|            0|      0|27.1|                   1.441| 57|    0|\n",
      "|          1|    189|           60|           23|    846|30.1|                   0.398| 59|    1|\n",
      "|          5|    166|           72|           19|    175|25.8|                   0.587| 51|    1|\n",
      "|          7|    100|            0|            0|      0|30.0|                   0.484| 32|    1|\n",
      "|          0|    118|           84|           47|    230|45.8|                   0.551| 31|    1|\n",
      "|          7|    107|           74|            0|      0|29.6|                   0.254| 31|    1|\n",
      "|          1|    103|           30|           38|     83|43.3|                   0.183| 33|    0|\n",
      "|          1|    115|           70|           30|     96|34.6|                   0.529| 32|    1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#carregando CSV\n",
    "diab = spark.read.load(\"pima-indians-diabetes.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "diab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|Class|\n",
      "+--------------------+-----+\n",
      "|[6.0,148.0,72.0,3...|    1|\n",
      "|[1.0,85.0,66.0,29...|    0|\n",
      "|[8.0,183.0,64.0,0...|    1|\n",
      "|[1.0,89.0,66.0,23...|    0|\n",
      "|[0.0,137.0,40.0,3...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler=VectorAssembler(inputCols=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "                          ,outputCol='features')\n",
    "\n",
    "output=assembler.transform(diab)\n",
    "output.select('features','Class').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              Class|\n",
      "+-------+-------------------+\n",
      "|  count|                545|\n",
      "|   mean| 0.3596330275229358|\n",
      "| stddev|0.48033368955231787|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data=output.select('features','Class')\n",
    "train_data,test_data=final_data.randomSplit([0.7,0.3])\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression(featuresCol='features', labelCol='Class')\n",
    "\n",
    "model=logr.fit(train_data)\n",
    "\n",
    "results=model.evaluate(train_data)\n",
    "  \n",
    "#print('Rsquared :',results.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|Class|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|    0|[5.35945491440936...|       0.0|[0.99531854985432...|\n",
      "|    0|[4.87422172956509...|       0.0|[0.99241690402990...|\n",
      "|    0|[3.56800453978200...|       0.0|[0.97256199004675...|\n",
      "|    0|[3.24942948595821...|       0.0|[0.96265260662548...|\n",
      "|    0|[2.19341948050555...|       0.0|[0.89965701966972...|\n",
      "|    0|[1.83141410404642...|       0.0|[0.86193010069208...|\n",
      "|    0|[2.21230456961086...|       0.0|[0.90134903635674...|\n",
      "|    0|[0.94492671843620...|       0.0|[0.72009375649918...|\n",
      "|    0|[1.50364478024666...|       0.0|[0.81811745314497...|\n",
      "|    0|[1.31786920128372...|       0.0|[0.78882697869835...|\n",
      "|    1|[2.22498336256877...|       0.0|[0.90247069858662...|\n",
      "|    0|[3.23431701449847...|       0.0|[0.96210545936005...|\n",
      "|    0|[1.92416277627702...|       0.0|[0.87260191862706...|\n",
      "|    0|[2.30500504579567...|       0.0|[0.90929070729634...|\n",
      "|    0|[1.99795160727787...|       0.0|[0.88058184206819...|\n",
      "|    0|[1.30989817067682...|       0.0|[0.78749611564810...|\n",
      "|    0|[2.73837516455440...|       0.0|[0.93925345556004...|\n",
      "|    0|[1.82211877360916...|       0.0|[0.86082016889306...|\n",
      "|    1|[1.34936284817868...|       0.0|[0.79402544215236...|\n",
      "|    0|[1.97898520396924...|       0.0|[0.87857294268765...|\n",
      "|    0|[1.82735546811538...|       0.0|[0.86144638633572...|\n",
      "|    1|[1.70373822714857...|       0.0|[0.84602233846638...|\n",
      "|    0|[1.66012159554678...|       0.0|[0.84025432513636...|\n",
      "|    0|[0.84549142928038...|       0.0|[0.69962051246714...|\n",
      "|    1|[0.51893738247595...|       0.0|[0.62689925724373...|\n",
      "|    1|[-1.4512035770271...|       1.0|[0.18981640345005...|\n",
      "|    1|[1.32107102974206...|       0.0|[0.78935984277117...|\n",
      "|    1|[0.81550261421222...|       0.0|[0.69328083685908...|\n",
      "|    0|[1.21982542948534...|       0.0|[0.77203282679976...|\n",
      "|    0|[0.77734810016461...|       0.0|[0.68510828757263...|\n",
      "|    1|[-2.2944947079751...|       1.0|[0.09157993440372...|\n",
      "|    0|[0.88131023941953...|       0.0|[0.70709366129688...|\n",
      "|    1|[-0.1247957829787...|       1.0|[0.46884148229303...|\n",
      "|    1|[-0.0025268542168...|       1.0|[0.49936828678189...|\n",
      "|    0|[-0.1220193869638...|       1.0|[0.46953294519690...|\n",
      "|    0|[-1.4121261159189...|       1.0|[0.19589892863732...|\n",
      "|    0|[-2.9852122249493...|       1.0|[0.04809842429487...|\n",
      "|    0|[4.06192528529119...|       0.0|[0.98307552726412...|\n",
      "|    0|[3.9463123986769,...|       0.0|[0.98104057032161...|\n",
      "|    0|[2.90791523350428...|       0.0|[0.94823633102688...|\n",
      "|    0|[3.67578875241422...|       0.0|[0.97529631108620...|\n",
      "|    0|[2.99311484535941...|       0.0|[0.95226210750108...|\n",
      "|    0|[2.28004195770694...|       0.0|[0.90721057892384...|\n",
      "|    0|[2.61225593887062...|       0.0|[0.93164619795015...|\n",
      "|    0|[2.18475248137414...|       0.0|[0.89887189806847...|\n",
      "|    0|[2.91854378653766...|       0.0|[0.94875554647584...|\n",
      "|    0|[3.09527871630644...|       0.0|[0.95669757609532...|\n",
      "|    0|[3.44314573575938...|       0.0|[0.96902607309561...|\n",
      "|    0|[3.67829162945272...|       0.0|[0.97535654226261...|\n",
      "|    0|[2.99893828119426...|       0.0|[0.95252613885919...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.select( 'Class', 'rawPrediction', 'prediction', 'probability').show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkKmeans').getOrCreate()\n",
    "\n",
    "df2 = spark.read.load(\"/home/silvio/dataset/minute_weather.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "                     \n",
    "df = df2.drop(\"rowID\",\"hpwren_timestamp\")\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "B=datetime.datetime.now()\n",
    "\n",
    "cost = []\n",
    "vecAssembler = VectorAssembler(inputCols=df.columns, outputCol=\"features\")\n",
    "vector_df = vecAssembler.transform(df)\n",
    "\n",
    "print('teste silhoute')    \n",
    "\n",
    "K = range(2,15)\n",
    "for k in K:\n",
    "    #kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol('features')\n",
    "    #model = kmeans.fit(vector_df)\n",
    "    kmeans = KMeans().setK(k).setSeed(1)\n",
    "    model = kmeans.fit(vector_df )\n",
    "    cost.append(model.summary.trainingCost)\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans as KM\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#from pyspark.ml.clustering import KMeans\n",
    "#from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "#from pyspark.ml.feature import VectorAssembler\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#spark = SparkSession.builder.appName('SparkKmeans').getOrCreate()\n",
    "\n",
    "df2 = pd.read_csv(\"/home/silvio/dataset/minute_weather.csv\")\n",
    "                     \n",
    "df = df2.drop(\"rowID\",\"hpwren_timestamp\")\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "B=datetime.datetime.now()\n",
    "\n",
    "cost = []\n",
    "#vecAssembler = VectorAssembler(inputCols=df.columns, outputCol=\"features\")\n",
    "#vector_df = vecAssembler.transform(df)\n",
    "    \n",
    "K = range(2,15)\n",
    "for k in K:\n",
    "    #kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol('features')\n",
    "    model = kmeans.fit(df)\n",
    "    #kmeans = KMeans().setK(k).setSeed(1)\n",
    "    #model = kmeans.fit(vector_df )\n",
    "    #cost.append(model.summary.trainingCost)\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "df2 = pd.read_csv(\"/home/silvio/dataset/minute_weather.csv\")\n",
    "                     \n",
    "df = df2.drop(\"rowID\",\"hpwren_timestamp\")\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "print('teste')\n",
    "# k means determine k\n",
    "silhouette = []\n",
    "K = range(2,16)\n",
    "for k in K:\n",
    "    print(k)\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(df)\n",
    "    kmeanModel.fit(df)\n",
    "    silhouette.append(silhouette_score(df, kmeanModel.labels_))\n",
    "    print(k)\n",
    "    \n",
    "# Plot the elbow\n",
    "#plt.plot(K, distortions, 'bx-')\n",
    "plt.plot(K, silhouette, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
