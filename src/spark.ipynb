{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componente de machine learning do Spark MLlib\n",
    "* Acesso a dados\n",
    "* Regressão, classificação e agrupamento com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ponto de entrada - sessão spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exemplo de operação com dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"pessoas.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algumas operações com dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema\n",
    "df.printSchema()\n",
    "\n",
    "# selecione apenas a coluna nome\n",
    "df.select(\"nome\").show()\n",
    "\n",
    "# seleciona todo mundo e adiciona 1 a idade\n",
    "df.select(df['nome'], df['idade'] + 1).show()\n",
    "\n",
    "# filtra apenas maiores de 21\n",
    "df.filter(df['idade'] > 21).show()\n",
    "\n",
    "# Conta pessoas por idade\n",
    "df.groupBy(\"idade\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Preparação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando CSV\n",
    "training = spark.read.load(\"Auto2.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Indexando para coluna categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer=StringIndexer(inputCol='origin',outputCol='origin_cat')\n",
    "indexed=indexer.fit(training).transform(training)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Separando entre features e target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#input cols identifica features\n",
    "assembler=VectorAssembler(inputCols=['cylinders','displacement','horsepower','weight','acceleration','year','origin_cat']\n",
    "                          ,outputCol='features')\n",
    "\n",
    "output=assembler.transform(indexed)\n",
    "output.select('features','mpg').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Separa em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_data=output.select('features','mpg')\n",
    "train_data,test_data=final_data.randomSplit([0.7,0.3])\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regressao linear com PySpark\n",
    "* Executa o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr=LinearRegression(featuresCol='features',labelCol='mpg')\n",
    "\n",
    "model=lr.fit(train_data)\n",
    "\n",
    "results=model.evaluate(train_data)\n",
    "  \n",
    "print('Rsquared :',results.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data=test_data.select('features')\n",
    "unlabeled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.transform(unlabeled_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classificação com pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando CSV\n",
    "diab = spark.read.load(\"pima-indians-diabetes.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "diab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler=VectorAssembler(inputCols=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "                          ,outputCol='features')\n",
    "\n",
    "output=assembler.transform(diab)\n",
    "output.select('features','Class').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=output.select('features','Class')\n",
    "train_data,test_data=final_data.randomSplit([0.7,0.3])\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression(featuresCol='features', labelCol='Class')\n",
    "\n",
    "model=logr.fit(train_data)\n",
    "\n",
    "results=model.evaluate(train_data)\n",
    "  \n",
    "#print('Rsquared :',results.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.select( 'Class', 'rawPrediction', 'prediction', 'probability').show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teste silhoute\n",
      "teste silhoute\n",
      "teste silhoute\n",
      "teste silhoute\n",
      "0:01:01.698701\n",
      "[31674549689.40811, 24087331264.325245, 19794348925.613716, 15439516264.466814, 9412087755.858288, 5057262331.923229, 3956913265.8476677, 3363326457.9555845, 3244683515.5098653, 2985889535.915539, 3432965682.942521, 2582613120.354102, 2266288466.903803]\n",
      "0:01:01.698701\n",
      "[31674549689.40811, 24087331264.325245, 19794348925.613716, 15439516264.466814, 9412087755.858288, 5057262331.923229, 3956913265.8476677, 3363326457.9555845, 3244683515.5098653, 2985889535.915539, 3432965682.942521, 2582613120.354102, 2266288466.903803]\n",
      "0:01:01.698701\n",
      "[31674549689.40811, 24087331264.325245, 19794348925.613716, 15439516264.466814, 9412087755.858288, 5057262331.923229, 3956913265.8476677, 3363326457.9555845, 3244683515.5098653, 2985889535.915539, 3432965682.942521, 2582613120.354102, 2266288466.903803]\n",
      "0:01:01.698701\n",
      "[31674549689.40811, 24087331264.325245, 19794348925.613716, 15439516264.466814, 9412087755.858288, 5057262331.923229, 3956913265.8476677, 3363326457.9555845, 3244683515.5098653, 2985889535.915539, 3432965682.942521, 2582613120.354102, 2266288466.903803]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkKmeans').getOrCreate()\n",
    "\n",
    "df2 = spark.read.load(\"/home/silvio/dataset/minute_weather.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "                     \n",
    "df = df2.drop(\"rowID\",\"hpwren_timestamp\")\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "B=datetime.datetime.now()\n",
    "\n",
    "cost = []\n",
    "vecAssembler = VectorAssembler(inputCols=df.columns, outputCol=\"features\")\n",
    "vector_df = vecAssembler.transform(df)\n",
    "\n",
    "print('teste silhoute')    \n",
    "\n",
    "K = range(2,15)\n",
    "for k in K:\n",
    "    #kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol('features')\n",
    "    #model = kmeans.fit(vector_df)\n",
    "    kmeans = KMeans().setK(k).setSeed(1)\n",
    "    model = kmeans.fit(vector_df )\n",
    "    cost.append(model.summary.trainingCost)\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
