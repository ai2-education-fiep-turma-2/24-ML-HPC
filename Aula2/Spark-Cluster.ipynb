{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import trim, isnan, isnull, when, count, col\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# criando sessão com Spark\n",
    "\n",
    "* Sessão pode ser local\n",
    "```\n",
    "    spark = SparkSession.builder.appName(\"stackoverflow\").getOrCreate()\n",
    "    ```\n",
    "    \n",
    "* Pode ser submetendo job para um cluster\n",
    "\n",
    "    ```\n",
    "    spark = SparkSession.builder.appName(\"stackoverflow\").master(\"spark://10.46.0.12:8893\").config(\"spark.driver.memory\",\"60G\").config(\"spark.executer.memory\", \"100g\").getOrCreate()\n",
    "    \n",
    "    ```\n",
    "* Alguns requisitos podem ser definidos na sessão (ex)\n",
    "    * spark.driver.memory\n",
    "    * spark.executer.memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"stackoverflow\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"stackoverflow\").config(\"spark.driver.memory\", \"100g\").config(\"spark.executer.memory\", \"100g\").config(\"spark.driver.maxResultSize\", \"50g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"stackoverflow\").master(\"local[5]\").config(\"spark.driver.memory\",\"60G\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"stackoverflow\").master(\"spark://10.46.0.12:8893\").config(\"spark.driver.memory\",\"60G\").config(\"spark.executer.memory\", \"100g\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando Dataframe:\n",
    "    * Operação ocorre de modo paralelizado, com base em recursos de Dataset e RDD do Spark de modo transparente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.686622\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "B=datetime.datetime.now()\n",
    "\n",
    "\n",
    "df_raw2 = spark.read.csv(\"/home/silvio/dataset/stackoverflow/train.csv\", escape='\"',multiLine=True,sep=',', ) #, escape='\"' )\n",
    "df_raw2.head()\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw2=df_raw2.filter(df_raw2['_c14'] != 'OpenStatus\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comando explain\n",
    "* Mostra organização interna so Spark para um objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embora seja otimizado, operações em dataframes não podem ser realizadas com comandos SQL, e algumas operações não são paralelizadas\n",
    "\n",
    "* As duas operações abaixo levam cerca de 30 segundos e rodam em um recurso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "B=datetime.datetime.now()\n",
    "print(df_raw2.select(\"_c2\").distinct().count())\n",
    "print((df_raw2.count(), len(df_raw2.columns)))\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma forma de otimizar operações em Dataframes é utilizar arquivos do tipo parquet que são visões do dados otimizada para acesso\n",
    "* Escrevendo arquivo parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw2.write.parquet(\"/home/silvio/stackOverflow2222.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lendo arquivo parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(\"/home/silvio/stackOverflow2222.parquet\")\n",
    "\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(parquetFile))\n",
    "print(type(df_raw2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parquet aceita qualquer operação SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spark.sql(\"SELECT count(_c2) FROM parquetFile\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spark.sql(\"SELECT count(distinct(_c2)) FROM parquetFile\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parquet aceita as operações de dataframe também que são executadas de modo otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c2\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquivos Parquet também são interoperáveis com Pandas (PyArrow)\n",
    "* Pré-processamento pode ser feito em Spark com desempenho e repassado ao pandas para facilidade de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Stack overflow\n",
    "* Diversas colunas caracterizando perguntas submetidas ao stackoverflow ( mais de 3 milhões de perguntas)\n",
    "* Desafio consiste em usar o texto das questões para predizer quais serão encerradas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processamento consiste em:\n",
    "* Eliminar valores nulos\n",
    "* Criar uma coluna texto com todo texto associado a questão\n",
    "* Vetorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c6\").head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c6\").head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c7\").head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c14\").head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c12\").tail(13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformando alvo em categoria ( a partir do parquet para ser mais rápido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.085822\n"
     ]
    }
   ],
   "source": [
    "B=datetime.datetime.now()\n",
    "\n",
    "indexer=StringIndexer(inputCol='_c14',outputCol='OpenStatus_cat')\n",
    "indexed=indexer.fit(parquetFile).transform(parquetFile)\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verificando valores nuloes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=datetime.datetime.now()\n",
    "\n",
    "df_raw3 = df_raw2.select(\"_c6\",\"_c7\",\"_c8\",\"_c9\",\"_c10\",\"_c11\",\"_c12\",\"_c14\")\n",
    "df_raw3.select([count(when(isnull(c), c)).alias(c) for c in df_raw3.columns]).show()\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eliminado campos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.118456\n"
     ]
    }
   ],
   "source": [
    "B=datetime.datetime.now()\n",
    "\n",
    "df_raw33 = indexed.fillna({'_c8':' '})\n",
    "df_raw34 = df_raw33.fillna({'_c9':' '})\n",
    "df_raw35 = df_raw34.fillna({'_c10':' '})\n",
    "df_raw36 = df_raw35.fillna({'_c11':' '})\n",
    "df_raw37 = df_raw36.fillna({'_c12':' '})\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw37.select([count(when(isnull(c), c)).alias(c) for c in df_raw37.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_raw37.count(), len(df_raw37.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unificando todos os campos de texto em um único chamado text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw4 = df_raw37.withColumn('text', sf.concat(sf.col('_c6'),sf.lit(' '), sf.col('_c7')\n",
    "                                               ,sf.lit(' '), sf.col('_c8')\n",
    "                                               ,sf.lit(' '), sf.col('_c9')\n",
    "                                               ,sf.lit(' '), sf.col('_c10')\n",
    "                                               ,sf.lit(' '), sf.col('_c11')\n",
    "                                               ,sf.lit(' '), sf.col('_c12')\n",
    "                                              ))\n",
    "\n",
    "df_raw5 = df_raw4.withColumn(\"text\", trim(df_raw4.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selecionando apenas campos de interesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw8 = df_raw5.select(\"text\",\"OpenStatus_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_raw8.count(), len(df_raw8.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw8 = df_raw8.withColumn(\"new_text\", F.array(F.col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw8.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vetorizando texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:33.406065\n"
     ]
    }
   ],
   "source": [
    "B=datetime.datetime.now()\n",
    "\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"new_text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(df_raw8)\n",
    "result = model.transform(df_raw8)\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)\n",
    "\n",
    "#for feature in result.select(\"result\").take(3):\n",
    "#    print(feature)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------------+\n",
      "|                text|OpenStatus_cat|            new_text|              result|\n",
      "+--------------------+--------------+--------------------+--------------------+\n",
      "|Decimal vs Double...|           0.0|[Decimal vs Doubl...|[-0.0022296668030...|\n",
      "|Percentage width ...|           0.0|[Percentage width...|[0.00485648820176...|\n",
      "|Tools for porting...|           0.0|[Tools for portin...|[-0.0025753492955...|\n",
      "|How do I calculat...|           0.0|[How do I calcula...|[-0.0032307314686...|\n",
      "|retrieve data fro...|           0.0|[retrieve data fr...|[-2.4345517886104...|\n",
      "|Reliable Timer in...|           0.0|[Reliable Timer i...|[-0.0010337549028...|\n",
      "|Fastest way to ge...|           0.0|[Fastest way to g...|[-0.0037973863072...|\n",
      "|Latest informatio...|           3.0|[Latest informati...|[0.00250316318124...|\n",
      "|Throw Error In My...|           0.0|[Throw Error In M...|[-0.0018924140604...|\n",
      "|How to use the C ...|           0.0|[How to use the C...|[3.49578273016959...|\n",
      "|How do I calculat...|           0.0|[How do I calcula...|[0.00194324378389...|\n",
      "|Determining web u...|           0.0|[Determining web ...|[0.00259029865264...|\n",
      "|What's the differ...|           0.0|[What's the diffe...|[-8.3940030890516...|\n",
      "|How do I fill a D...|           0.0|[How do I fill a ...|[0.00447172066196...|\n",
      "|Binary Data in MY...|           0.0|[Binary Data in M...|[-2.5824963813647...|\n",
      "|Best way to allow...|           0.0|[Best way to allo...|[3.32897907355800...|\n",
      "|Multiple submit b...|           0.0|[Multiple submit ...|[0.00422647641971...|\n",
      "|How do I get a di...|           0.0|[How do I get a d...|[3.70030989870429...|\n",
      "|How do you page a...|           0.0|[How do you page ...|[-0.0039453203789...|\n",
      "|How do I add exis...|           0.0|[How do I add exi...|[7.29836814571172...|\n",
      "+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              result|OpenStatus_cat|\n",
      "+--------------------+--------------+\n",
      "|[-0.0022296668030...|           0.0|\n",
      "|[0.00485648820176...|           0.0|\n",
      "|[-0.0025753492955...|           0.0|\n",
      "|[-0.0032307314686...|           0.0|\n",
      "|[-2.4345517886104...|           0.0|\n",
      "|[-0.0010337549028...|           0.0|\n",
      "|[-0.0037973863072...|           0.0|\n",
      "|[0.00250316318124...|           3.0|\n",
      "|[-0.0018924140604...|           0.0|\n",
      "|[3.49578273016959...|           0.0|\n",
      "|[0.00194324378389...|           0.0|\n",
      "|[0.00259029865264...|           0.0|\n",
      "|[-8.3940030890516...|           0.0|\n",
      "|[0.00447172066196...|           0.0|\n",
      "|[-2.5824963813647...|           0.0|\n",
      "|[3.32897907355800...|           0.0|\n",
      "|[0.00422647641971...|           0.0|\n",
      "|[3.70030989870429...|           0.0|\n",
      "|[-0.0039453203789...|           0.0|\n",
      "|[7.29836814571172...|           0.0|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultF=result.select(\"result\",\"OpenStatus_cat\")\n",
    "resultF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|     OpenStatus_cat|\n",
      "+-------+-------------------+\n",
      "|  count|            2359083|\n",
      "|   mean|0.04099770970330421|\n",
      "| stddev|0.31625030153069295|\n",
      "|    min|                0.0|\n",
      "|    max|                4.0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data=resultF.select('result','OpenStatus_cat')\n",
    "train_data,test_data=final_data.randomSplit([0.7,0.3])\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"OpenStatus_cat\", featuresCol=\"result\")\n",
    "\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|prediction|OpenStatus_cat|              result|\n",
      "+----------+--------------+--------------------+\n",
      "|       0.0|           0.0|[-0.0049999011680...|\n",
      "|       0.0|           0.0|[-0.0049997083842...|\n",
      "|       0.0|           0.0|[-0.0049996017478...|\n",
      "|       0.0|           0.0|[-0.0049992250278...|\n",
      "|       0.0|           0.0|[-0.0049992180429...|\n",
      "+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.0206259 \n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "predictions.select(\"prediction\", \"OpenStatus_cat\", \"result\").show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"OpenStatus_cat\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[0]\n",
    "\n",
    "#print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
