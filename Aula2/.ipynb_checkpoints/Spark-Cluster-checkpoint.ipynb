{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import trim, isnan, isnull, when, count, col\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# criando sessão com Spark\n",
    "\n",
    "* Sessão pode ser local\n",
    "```\n",
    "    spark = SparkSession.builder.appName(\"stackoverflow\").getOrCreate()\n",
    "    ```\n",
    "    \n",
    "* Pode ser submetendo job para um cluster\n",
    "\n",
    "    ```\n",
    "    spark = SparkSession.builder.appName(\"stackoverflow\").master(\"spark://10.46.0.12:8893\").config(\"spark.driver.memory\",\"60G\").config(\"spark.executer.memory\", \"100g\").getOrCreate()\n",
    "    \n",
    "    ```\n",
    "* Alguns requisitos podem ser definidos na sessão (ex)\n",
    "    * spark.driver.memory\n",
    "    * spark.executer.memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"stackoverflow\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"stackoverflow\").config(\"spark.driver.memory\", \"100g\").config(\"spark.executer.memory\", \"100g\").config(\"spark.driver.maxResultSize\", \"4g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"stackoverflow\").master(\"local[5]\").config(\"spark.driver.memory\",\"60G\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName(\"stackoverflow\").master(\"spark://10.46.0.12:8893\").config(\"spark.driver.memory\",\"60G\").config(\"spark.executer.memory\", \"100g\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando Dataframe:\n",
    "    * Operação ocorre de modo paralelizado, com base em recursos de Dataset e RDD do Spark de modo transparente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "B=datetime.datetime.now()\n",
    "\n",
    "\n",
    "df_raw2 = spark.read.csv(\"/home/silvio/dataset/stackoverflow/train.csv\", escape='\"',multiLine=True,sep=',', ) #, escape='\"' )\n",
    "df_raw2.head()\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw2=df_raw2.filter(df_raw2['_c14'] != 'OpenStatus\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comando explain\n",
    "* Mostra organização interna so Spark para um objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw2.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embora seja otimizado, operações em dataframes não podem ser realizadas com comandos SQL, e algumas operações não são paralelizadas\n",
    "\n",
    "* As duas operações abaixo levam cerca de 30 segundos e rodam em um recurso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "B=datetime.datetime.now()\n",
    "print(df_raw2.select(\"_c2\").distinct().count())\n",
    "print((df_raw2.count(), len(df_raw2.columns)))\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma forma de otimizar operações em Dataframes é utilizar arquivos do tipo parquet que são visões do dados otimizada para acesso\n",
    "* Escrevendo arquivo parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw2.write.parquet(\"/home/silvio/stackOverflow222.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lendo arquivo parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(\"/home/silvio/stackOverflow222.parquet\")\n",
    "\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(parquetFile))\n",
    "print(type(df_raw2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parquet aceita qualquer operação SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spark.sql(\"SELECT count(_c2) FROM parquetFile\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spark.sql(\"SELECT count(distinct(_c2)) FROM parquetFile\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parquet aceita as operações de dataframe também que são executadas de modo otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c2\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquivos Parquet também são interoperáveis com Pandas (PyArrow)\n",
    "* Pré-processamento pode ser feito em Spark com desempenho e repassado ao pandas para facilidade de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Stack overflow\n",
    "* Diversas colunas caracterizando perguntas submetidas ao stackoverflow ( mais de 3 milhões de perguntas)\n",
    "* Desafio consiste em usar o texto das questões para predizer quais serão encerradas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processamento consiste em:\n",
    "* Eliminar valores nulos\n",
    "* Criar uma coluna texto com todo texto associado a questão\n",
    "* Vetorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c6\").head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c6\").head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c7\").head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c14\").head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parquetFile.select(\"_c12\").tail(13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformando alvo em categoria ( a partir do parquet para ser mais rápido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=datetime.datetime.now()\n",
    "\n",
    "indexer=StringIndexer(inputCol='_c14',outputCol='OpenStatus_cat')\n",
    "indexed=indexer.fit(parquetFile).transform(parquetFile)\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verificando valores nuloes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=datetime.datetime.now()\n",
    "\n",
    "df_raw3 = df_raw2.select(\"_c6\",\"_c7\",\"_c8\",\"_c9\",\"_c10\",\"_c11\",\"_c12\",\"_c14\")\n",
    "df_raw3.select([count(when(isnull(c), c)).alias(c) for c in df_raw3.columns]).show()\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eliminado campos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=datetime.datetime.now()\n",
    "\n",
    "df_raw33 = indexed.fillna({'_c8':' '})\n",
    "df_raw34 = df_raw33.fillna({'_c9':' '})\n",
    "df_raw35 = df_raw34.fillna({'_c10':' '})\n",
    "df_raw36 = df_raw35.fillna({'_c11':' '})\n",
    "df_raw37 = df_raw36.fillna({'_c12':' '})\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw37.select([count(when(isnull(c), c)).alias(c) for c in df_raw37.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_raw37.count(), len(df_raw37.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unificando todos os campos de texto em um único chamado text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw4 = df_raw37.withColumn('text', sf.concat(sf.col('_c6'),sf.lit(' '), sf.col('_c7')\n",
    "                                               ,sf.lit(' '), sf.col('_c8')\n",
    "                                               ,sf.lit(' '), sf.col('_c9')\n",
    "                                               ,sf.lit(' '), sf.col('_c10')\n",
    "                                               ,sf.lit(' '), sf.col('_c11')\n",
    "                                               ,sf.lit(' '), sf.col('_c12')\n",
    "                                              ))\n",
    "\n",
    "df_raw5 = df_raw4.withColumn(\"text\", trim(df_raw4.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selecionando apenas campos de interesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw8 = df_raw5.select(\"text\",\"OpenStatus_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_raw8.count(), len(df_raw8.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw8 = df_raw8.withColumn(\"new_text\", F.array(F.col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw8.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vetorizando texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=datetime.datetime.now()\n",
    "\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"new_text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(df_raw8)\n",
    "result = model.transform(df_raw8)\n",
    "\n",
    "E=datetime.datetime.now()\n",
    "print(E-B)\n",
    "\n",
    "#for feature in result.select(\"result\").take(3):\n",
    "#    print(feature)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultF=result.select(\"result\",\"OpenStatus_cat\")\n",
    "resultF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=resultF.select('result','OpenStatus_cat')\n",
    "train_data,test_data=final_data.randomSplit([0.7,0.3])\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"OpenStatus_cat\", featuresCol=\"result\")\n",
    "\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "predictions.select(\"prediction\", \"OpenStatus_cat\", \"result\").show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"OpenStatus_cat\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[0]\n",
    "\n",
    "#print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
